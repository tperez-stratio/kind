= Operations manual

== Generation of custom images

For the generation of custom images in the different cloud providers, some reference documentation has been created for each of them and can be found at the following links:

* xref:operations-manual:image-builder/aws-image-builder.adoc[AWS]
* xref:operations-manual:image-builder/azure-image-builder.adoc[Azure]
* xref:operations-manual:image-builder/gcp-image-builder.adoc[GCP]

== Obtaining the _kubeconfig_

To communicate with the API Server of the created cluster, the _kubeconfig_ file is necessary, which will be obtained differently depending on the cloud provider used and the _control-plane_ management of the cluster.

* For EKS, it will be obtained as indicated by AWS:
+
[source,bash]
----
aws eks update-kubeconfig --region eu-west-1 --name <cluster_name> --kubeconfig ./<cluster_name>.kubeconfig
----

* For GCP, unmanaged Azure and AKS, at the end of provisioning, the _kubeconfig_ is left in the workspace directory:

[source,bash]
----
ls ./.kube/config
./.kube/config
----
+
In turn, the alias "kw" may be used from the local container to interact with the cluster _worker_ (in EKS, the token used only lasts for 10 minutes):
+
[source,bash]
----
root@example-azure-control-plane:/# kw get nodes
NAME STATUS ROLES AGE VERSION
example-azure-control-plane-6kp94 Ready control-plane 60m v1.26.8
example-azure-control-plane-fgkcc Ready control-plane 63m v1.26.8
...
----

== Authentication in EKS

While not part of the _Stratio KEOS_ operation, it is important to highlight how to enable https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html[authentication of other users in an EKS cluster] (the cluster creator user is authenticated by default).

To give Kubernetes-admin permissions on the cluster, the user's ARN will be added in the _ConfigMap_ given below.

[source,bash]
----
$ kubectl -n kube-system edit cm aws-auth
..
data:
  mapUsers: |
    - groups:
      - system:masters
      userarn: <user_arn>
      username: kubernetes-admin
----

== Infrastructure operation

image::controllers.png[]

_Stratio KEOS_ allows multiple advanced operations to be performed by interacting with the _Stratio Cluster Operator_ (infrastructure as code or IaC), which in its reconciliation cycle in turn interacts with the different providers to perform the requested operations.

=== Self-healing

image::self-healing.png[]

The self-healing capability of the cluster is managed by the _MachineHealthCheck_ object:

[source,bash]
----
$ kubectl -n cluster-example get mhc -o yaml
...
  spec:
    clusterName: example
    maxUnhealthy: 100%
    nodeStartupTimeout: 5m0s
    selector:
      matchLabels:
        keos.stratio.com/machine-role: example-worker-node
    unhealthyConditions:
    - status: Unknown
      timeout: 1m0s
      type: Ready
    - status: "False"
      timeout: 1m0s
      type: Ready
...
----

NOTE: Unmanaged providers will have one _MachineHealthCheck_ for the _control-plane_ and another for the _worker_ nodes, while managed ones (EKS, AKS) will only have the second one.

==== Failover test on a node

In case of failure in a node, it will be detected by a controller and it will be replaced by deleting it and recreating another one of the same group, which ensures the same characteristics.

To simulate a VM failure, it will be deleted from the cloud provider's web console.

The recovery of the node comprises the following phases and estimated times (which may vary depending on the provider and the flavour):

[source,bash]
----
. Terminate VM from console: 0s
. New VM is Provisioning: 50s
. Old Machine is Deleted & the new one is Provisioned: 1m5s
. New Machine is Running & new k8s node is NotReady: 1m 50s
. New k8s node is Ready: 2m
----

=== Static scaling

Although manual scaling of an existing node group is discouraged, these operations are provided for cases without autoscaling or new node groups.

==== Scaling a _workers_ group

image::escalado-manual.png[]

To manually scale a group of _workers_ the _KeosCluster_ object is used:

[source,bash]
----
kubectl -n cluster-example-eks edit keoscluster
----

[source,yaml]
----
spec:
  ...
  worker_nodes:
    - name: eks-prod-xlarge
      quantity: 9
      ...
----

Verify the change by querying the state of the _KeosCluster_ object:

[source,bash]
----
kubectl -n cluster-<cluster_name> get keoscluster <cluster_name> --subresource=status
----

===== AKS

In the case of this provider, if `min_size` and `max_size` parameters are defined in the _KeosCluster_ object, no action is performed.

NOTE: The node groups of the _KeosCluster_ object correspond in Azure to _Node pools_ within AKS and their corresponding _VM Scale Sets_.

Manual scaling of a node pool in AKS with autoscaling configured should be done from the Azure portal under:

'VM Scale set' -> '<scale_set_name>' -> 'Scalling' -> '<instance_number>'

or from:

'Kubernetes services' -> '<aks_name>' -> 'Node pools' -> '<nodepool_name>' -> 'Scale node pool' -> 'Manual' -> '<node_count>'

The new instances can be seen in 'VM Scale set' -> 'Instances'. This change will not be reflected in the `quantity` parameter of the node pool of the _KeosCluster_ object.

The estimated times for this process are as follows:

[source,bash]
----
Scale VM Scale set: 0s
New K8s node is NotReady: 1m
New K8s node is Ready: 1m 13s
The MachinePool Scaling: 1m 29s
The MachinePool is updated: 1m 33s
----

==== Create a new workers group

To create a new group of nodes just create a new element to the array _worker++_++nodes_ of the _KeosCluster_ object:

[source,yaml]
----
spec:
  ...
  worker_nodes:
    - ...
    - name: eks-prod-xlarge
      quantity: 6
      max_size: 18
      min_size: 6
      size: m6i.xlarge
      labels:
        disktype: standard
      root_volume:
        size: 50
        type: gp3
        encrypted: true
      ssh_key: stg-key
----

Again, verify the change by querying the state of the _KeosCluster_ object:

[source,bash]
----
kubectl -n cluster-<cluster_name> get keoscluster <cluster_name> --subresource=status
----

==== Vertical scaling

CAUTION: *AKS does not support vertical scaling* of node groups. For this provider, you must create a new group and delete the previous one as indicated in the https://learn.microsoft.com/en-us/azure/aks/resize-node-pool[official documentation].

The vertical scaling of a node group is done by modifying the instance type in the _KeosCluster_ object corresponding to the group.

[source,yaml]
----
spec:
  ...
  worker_nodes:
    - name: eks-prod-xlarge
      size: m6i.2xlarge
      ...
----

=== Autoscaling

image::autoescalado.png[]

For node autoscaling, _cluster-autoscaler_ is used, which will detect pods pending execution due to a lack of resources and will scale groups of nodes according to the deployment filters.

This operation is performed in the API Server, being the controllers in charge of creating the VMs in the cloud provider and adding them to the cluster as Kubernetes _worker_ nodes.

Since the autoscaling is based on the _cluster-autoscaler_, the minimum and maximum will be added to the node group in the _KeosCluster_ object:

[source,yaml]
----
spec:
  ...
  worker_nodes:
    - name: eks-prod-xlarge
      min_size: 6
      max_size: 21
      ...
----

===== AKS

In this provider the autoscaling is managed from the _VM Scale sets_ of Azure and not with the _cluster-autoscaler_.

During provisioning, at the time of creating the node pools, the _Node pools_ will be instantiated in AKS and their respective _VM Scale Sets_. If the defined node pools have an auto-scaling range, these will be moved to the created _Node pools_.

To view them in the Azure portal, you should refer to:

'Kubernetes services' -> 'aks_name>' -> 'Node pools' -> '<nodepool_name>' -> 'Scale node pool' -> 'Autoscale'.

==== Test

To test autoscaling, you can create a deployment with enough replicas to prevent them from running on the current nodes:

[source,bash]
----
kubectl create deploy test --replicas 1500 --image nginx:alpine
----

At the end of the test, remove the deployment:

[source,bash]
----
kubectl --kubeconfig demo-eks.kubeconfig delete deploy test
----

==== Logs

The logs of the _cluster-autoscaler_ can be viewed from its deployment:

[source,bash]
----
kubectl -n kube-system logs -f -l app.kubernetes.io/name=clusterapi-cluster-autoscaler
----

=== _Stratio Cloud Provisioner_ upgrade to 0.5

==== Before you start

Unlike other versions of the upgrade script, all steps until the upgrade is complete are done automatically and without human interaction except for commits and version information requests. During the upgrade, it is important to note that a consecutive Kubernetes version change will be performed, following the automated procedure below:

- Perform a backup process of the objects that manage the infrastructure.
- Upgrade and/or add services within the Kubernetes cluster.
- Upgrade the Kubernetes version from 1.26.X to 1.27.X.
- Upgrade the version of Kubernetes from 1.27.X to 1.28.X.
- Restore, if necessary, the services from the backup process previously performed.

Regarding the Kubernetes version upgrade, this will be done in two stages within the same atomic process: first, the _control-plane_, and once on the new version, the _worker_ nodes, iterating through each group of nodes and upgrading them one by one sequentially.

CAUTION: Upgrading the Kubernetes version of nodes in clusters where the image has not been specified may involve an OS upgrade.

==== Prerequisites

* The following binaries must be available on the bastion machine:
** python3
** ansible-vault (pip)
** clusterctl
** helm
** kubectl
** jq
** aws (optional)
** az (optional)
** gcloud (optional)
* The necessary permissions must be ensured in the backup directory of the bastion machine so that the user running the script can write to it (the _./backup/upgrade/_ directory is created).
* Kubernetes version compatibility must be ensured for different vendors, with special attention for clusters whose node groups do not have a custom image defined:
** EKS
+
[source,bash]
----
aws eks describe-addon-versions | jq -r ".addons[] | .addonVersions[] | .compatibilities[] | .clusterVersion" | sort -nr | uniq | head -4
----

* AWS VMs
+
[source,bash]
----
aws ec2 describe-images --filters "Name=name,Values=capa-ami-ubuntu-18.04-*" --query 'Images[*].{ID:ImageId,Name:Name}' --output table
----

* AKS
+
[source,bash]
----
az aks get-versions --location <region> --output table
----

* Azure VMs
+
[source,bash]
----
az vm image list --publisher cncf-upstream --offer "capi" --sku ubuntu-2204 --all -o table
----

* GCP VMs. It is not applicable as it is mandatory to specify a custom image.
* Extreme caution should be exercised in production environments, especially on unmanaged providers. In particular, before upgrading, it is recommended to make a backup of the objects that manage the infrastructure and services considered critical.

==== Execution

During the execution of the script, the user will be required to confirm the continuation of the process at various stages and provide relevant information, such as the version of Kubernetes to be upgraded.

The script `upgrade-provisioner_.py` must be run, help for which can be found with the following command:

[source,bash]
----
python3 upgrade-provisioner.py -h
----

Basic example:

[source,bash]
----
python3 upgrade-provisioner.py -p <vault_pass>
----

Example output of the execution:

[source,bash]
----
[INFO] Using kubeconfig: /tmp/kubeconfig
[INFO] Cluster name: esierra-dev-vms
Press ENTER to continue upgrading the cluster or any other key to abort: 
[INFO] Verifying upgrade process
[INFO] Backing up files into directory ./backup/upgrade/20240611-132257
[INFO] Backing up CAPX files: OK
[INFO] Backing up capsule files: OK
[INFO] Preparing capsule-mutating-webhook-configuration for the upgrade process: OK
[INFO] Preparing capsule-validating-webhook-configuration for the upgrade process: OK
[INFO] Applying new ClusterConfig CRD: OK
[INFO] Upgrading Cluster Operator 0.3.0: OK
[INFO] Restoring capsule-mutating-webhook-configuration: OK
[INFO] Restoring capsule-validating-webhook-configuration: OK
Please provide the Kubernetes version to which you want to upgrade: 1.27.11
Are you sure you want to upgrade to version 1.27.11? (yes/no): y
[INFO] Initiating upgrade to kubernetes to version 1.27.11
[INFO] Scaling down cluster autoscaler replicas: OK
[INFO] Applying temporal allow control plane GlobalNetworkPolicy: OK
Please provide the image ID associated with the Kubernetes version: 1.27.11 for control-plane: ami-09d1a38098ccd9d16
Are you sure you want to use node image: ami-09d1a38098ccd9d16 for control-plane? (yes/no): y 
Please provide the image ID associated with the Kubernetes version: 1.27.11 for worker node: worker1: ami-09d1a38098ccd9d16
Are you sure you want to use node image: ami-09d1a38098ccd9d16 for worker node: worker1? (yes/no): y
Please provide the image ID associated with the Kubernetes version: 1.27.11 for worker node: minmax0: ami-09d1a38098ccd9d16
Are you sure you want to use node image: ami-09d1a38098ccd9d16 for worker node: minmax0? (yes/no): y
[INFO] node_image is not defined in worker node: noimage
[INFO] Waiting for the Kubernetes version upgrade - control plane: OK
[INFO] Waiting for the Kubernetes version upgrade - worker nodes: OK
[INFO] Restoring allow control plane GlobalNetworkPolicy: OK
[INFO] Scaling up cluster autoscaler replicas: OK
Please provide the Kubernetes version to which you want to upgrade: 1.28.7
Are you sure you want to upgrade to version 1.28.7? (yes/no): y
[INFO] Initiating upgrade to kubernetes to version 1.28.7
[INFO] Scaling down cluster autoscaler replicas: OK
[INFO] Applying temporal allow control plane GlobalNetworkPolicy: OK
Please provide the image ID associated with the Kubernetes version: 1.28.7 for control-plane: ami-0a226d9d637560c4b
Are you sure you want to use node image: ami-0a226d9d637560c4b for control-plane? (yes/no): yes
Please provide the image ID associated with the Kubernetes version: 1.28.7 for worker node: worker1: ami-0a226d9d637560c4b
Are you sure you want to use node image: ami-0a226d9d637560c4b for worker node: worker1? (yes/no): yes
Please provide the image ID associated with the Kubernetes version: 1.28.7 for worker node: minmax0: ami-0a226d9d637560c4b
Are you sure you want to use node image: ami-0a226d9d637560c4b for worker node: minmax0? (yes/no): yes
[INFO] node_image is not defined in worker node: noimage
[INFO] Waiting for the Kubernetes version upgrade - control plane: OK
[INFO] Waiting for the Kubernetes version upgrade - worker nodes: OK
[INFO] Restoring allow control plane GlobalNetworkPolicy: OK
[INFO] Scaling up cluster autoscaler replicas: OK
[INFO] Upgrade process finished successfully in 117 minutes and 14.68 seconds
----

In case of failure, since it is an idempotent script, you can run it as many times as you want and get the successful update completion message.

==== Post-upgrade verification

===== etcd

One way to ensure that the etcd is correct after upgrading an unmanaged _control-plane_ is to open a terminal on any pod of etcd, view the _cluster_ status, and compare the IPs of the registered members with those of the _control-plane_ nodes.

[source,bash]
----
kubectl -n kube-system exec -ti etcd-<control-plane-node> sh

alias e="etcdctl --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --cacert=/etc/kubernetes/pki/etcd/ca.crt "
e endpoint status
e endpoint status -w table --cluster
e member list
e member remove <member-id>
----

===== cluster-autoscaler

Check that the _deployment_ of cluster-autoscaler is configured with its original number of replicas, that is, equal to 2.

[source,bash]
----
kubectl get deploy cluster-autoscaler-clusterapi-cluster-autoscaler -n kube-system -ojsonpath='{.status.replicas}''.
----

=== Cluster removal

[NOTE]
.Preliminary considerations
====
Before deleting the cloud provider resources generated by _Stratio Cloud Provisioner_ you must delete those that have been created by the _keos-installer_ or any external automatism (for example, the _Services_ of type _LoadBalancer_).

Also, you should note that the process requires the _clusterctl_ binary on the bastion machine (any computer with access to the _API Server_) on which it will run.
====

Run the following steps to perform the cluster removal:

. Create a local cluster indicating that no object is generated in the cloud provider.
+
[source,bash]
----
[bastion]$ sudo ./bin/cloud-provisioner create cluster --name <cluster_name> --descriptor cluster.yaml --vault-password <my-passphrase> --avoid-creation
----

. Pause the controller of the _Stratio Cluster Operator_:
+
[source,bash]
----
[bastion]$ kubectl --kubeconfig $KUBECONFIG -n kube-system scale deployment keoscluster-controller-manager --replicas 0
----

. Move the cluster _worker_ management to the local cluster using the corresponding _kubeconfig_ (note that for managed _control-planes_ the _kubeconfig_ of the provider will be needed). To ensure this step, look for the following text in the command output: "Moving Cluster API objects Clusters=1".
+
[source,bash]
----
[bastion]$ sudo clusterctl --kubeconfig $KUBECONFIG move -n cluster-<cluster_name> --to-kubeconfig /root/.kube/config
----

. Access the local cluster and delete the cluster _worker_.
+
[source,bash]
----
[bastion]$ sudo docker exec -ti <nombre_cluster>-control-plane bash
root@<nombre_del_cluster>-control-plane:/# kubectl -n cluster-<nombre_del_cluster> delete cl --all
cluster.cluster.x-k8s.io "<nombre_del_cluster>" eliminado
root@<nombre_del_cluster>-plano-de-control:/#
----

. Finally, remove the local cluster.
+
[source,bash]
----
[bastion]$ sudo ./bin/cloud-provisioner delete cluster --name <nombre_cluster>
----

== Offline installation

To learn how to perform an installation where the workload images of the cluster come from repositories accessible from environments without internet access, see the xref:operations-manual:offline-installation.adoc[offline installation manual].
