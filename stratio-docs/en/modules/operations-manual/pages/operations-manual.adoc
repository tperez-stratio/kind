= Operations manual

== Obtaining the _kubeconfig_

To communicate with the API Server of the created cluster, the _kubeconfig_ file is necessary, which will be obtained differently depending on the cloud provider used and the _control-plane_ management of the cluster.

* For EKS, it will be obtained as indicated by AWS:
+
[source,bash]
----
aws eks update-kubeconfig --region eu-west-1 --name <cluster_name> --kubeconfig ./<cluster_name>.kubeconfig
----

* For GKE, the credentials will be obtained as specified by GCP:
+
[source,bash]
----
gcloud container clusters get-credentials <cluster_name> --region <region> --project <project>
----

This command will generate the file containing the _kubeconfig_ in the location specified by the `KUBECONFIG` environment variable. By default, it will be `$HOME/.kube/config`.

* For unmanaged Azure, at the end of provisioning, the _kubeconfig_ is left in the workspace directory:

[source,bash]
----
ls ./.kube/config
./.kube/config
----
+
In turn, the alias "kw" may be used from the local container to interact with the cluster _worker_ (in EKS, the token used only lasts for 10 minutes):
+
[source,bash]
----
root@example-azure-control-plane:/# kw get nodes
NAME STATUS ROLES AGE VERSION
example-azure-control-plane-6kp94 Ready control-plane 60m v1.26.8
example-azure-control-plane-fgkcc Ready control-plane 63m v1.26.8
...
----

== Authentication in EKS

While not part of the _Stratio KEOS_ operation, it is important to highlight how to enable https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html[authentication of other users in an EKS cluster] (the cluster creator user is authenticated by default).

To give Kubernetes-admin permissions on the cluster, the user's ARN will be added in the _ConfigMap_ given below.

[source,bash]
----
$ kubectl -n kube-system edit cm aws-auth
..
data:
  mapUsers: |
    - groups:
      - system:masters
      userarn: <user_arn>
      username: kubernetes-admin
----

== Enable _Assume Role_ authorization in AWS for an EKS cluster

This procedure describes how to enable the _assume role_â€“based authorization method in AWS for an EKS cluster after it has been created.

=== Prerequisites

Before you begin, make sure of the following:

* You have created an IAM role with the required permissions (e.g. _stratio-assume-role_).
** Role type: _AWS Account_.
** Permissions: the same as those described in the EKS prerequisites.
* The role's _trust relationship_ is configured to allow the user who deployed the cluster to assume the role.
+
Example _trust relationship_:
+
[source,json]
----
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::123456789012:user/stratio-user"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
----

* The user is assigned a policy that includes the `sts:AssumeRole` action for the corresponding role.
+
Example policy:
+
[source,json]
----
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "sts:AssumeRole",
      "Resource": "arn:aws:iam::123456789012:role/stratio-assume-role"
    }
  ]
}
----

NOTE: The minimum supported version of _cluster-operator_ for this method is 0.5.2.

=== Configure _assume role_ in the EKS cluster

NOTE: Replace the account, cluster name `eks-cl01`, and namespace `cluster-eks-cl01` with the values used in your environment.

. Create the `AWSClusterRoleIdentity` object.
+
[source,yaml]
----
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: AWSClusterRoleIdentity
metadata:
  name: eks-cl01-role-identity
spec:
  roleARN: arn:aws:iam::123456789012:role/stratio-assume-role
  sessionName: eks-cl01-role-identity-session
  durationSeconds: 3600
  sourceIdentityRef:
    kind: AWSClusterControllerIdentity
    name: default
  allowedNamespaces:
    list:
    - cluster-eks-cl01
----

. Associate the `AWSClusterRoleIdentity` with the `AWSManagedControlPlane`.
+
[source,bash]
----
kubectl patch awsmanagedcontrolplane eks-cl01-control-plane \
  -n cluster-eks-cl01 \
  --type='merge' \
  -p '{"spec":{"identityRef":{"kind":"AWSClusterRoleIdentity","name":"eks-cl01-role-identity"}}}'
----

. Restart the `capa-controller-manager` deployment to apply the changes:
+
[source,bash]
----
kubectl rollout restart deployment capa-controller-manager -n capa-system
----

. Configure `aws-auth` with the `iam_role`.
+
[source,yaml]
----
kubectl patch configmap aws-auth -n kube-system --type merge -p '
data:
  mapRoles: |
    - groups:
      - system:bootstrappers
      - system:nodes
      rolearn: arn:aws:iam::123456789012:role/nodes.cluster-api-provider-aws.sigs.k8s.io
      username: system:node:{{EC2PrivateDNSName}}
    - groups:
      - capa-manager
      rolearn: arn:aws:iam::963353512345678901211234:role/stratio-assume-role
      username: stratio-assume-role
'
----

. Create the `ClusterRoleBinding` for the `capa-manager` group.
+
[source,yaml]
----
kubectl apply -f - <<EOF
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: capa-manager-access
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: capa-manager-role
subjects:
- kind: Group
  name: capa-manager
  apiGroup: rbac.authorization.k8s.io
EOF
----

. Add permissions to the `ClusterRole` for the `capa-manager` group.
+
[source,bash]
----
kubectl patch clusterrole capa-manager-role \
  --type='json' \
  -p='[
    {"op": "add", "path": "/rules/-", "value": {"apiGroups":["apps"],"resources":["daemonsets"],"verbs":["get","list","watch","update"]}},
    {"op": "add", "path": "/rules/-", "value": {"apiGroups":[""],"resources":["pods"],"verbs":["get","list","watch"]}},
    {"op": "add", "path": "/rules/-", "value": {"apiGroups":[""],"resources":["nodes"],"verbs":["get","list","watch","patch"]}},
    {"op": "add", "path": "/rules/-", "value": {"apiGroups":[""],"resources":["pods/eviction"],"verbs":["create"]}}
  ]'
----

. Update the _cluster-operator_.
.. Update the _helmrelease_ associated with _cluster-operator_ to version 0.5.2 or later.
.. Update the corresponding _ConfigMap_ for that version.
.. Patch the _keoscluster-settings_ secret to include the `role_arn`.
+
[source,bash]
-----
kubectl -n kube-system patch secret keoscluster-settings \
  --type=json \
  -p='[{"op":"replace","path":"/data/credentials","value":"'$(kubectl -n kube-system get secret keoscluster-settings -o jsonpath="{.data.credentials}" | base64 -d | awk 'BEGIN{ORS="\n"} {print} END{print "role_arn = arn:aws:iam::123456789012:role/stratio-assume-role"}' | base64 -w0)'"}]'
-----

. Verify the configuration and permissions:
+
[source,bash]
----
# Check capi/capa logs
kubectl logs -f -n capa-system deployment/capa-controller-manager (o el nombre del pod)
kubectl logs -f -n capi-system deployment/capi-controller-manager (o el nombre del pod)

# Check _cluster-operator_ logs
kubectl logs -f -n capa-system deployment/keoscluster-controller-manager (o el nombre del pod)

# Check status and configuration
kubectl get awsclusterroleidentity
kubectl get awsmanagedcontrolplane -n cluster-eks-cl01
kubectl get configmap aws-auth -n kube-system -o yaml

# Verify role permissions
kubectl auth can-i get nodes --as=stratio-assume-role --as-group=capa-manager
kubectl auth can-i list nodes --as=stratio-assume-role --as-group=capa-manager
kubectl auth can-i list pods --as=stratio-assume-role --as-group=capa-manager
kubectl auth can-i update daemonsets --as=stratio-assume-role --as-group=capa-manager
----

=== Cluster operations using _assume role_

Once _assume role_ is enabled, you can also manage the cluster from the command line by following these steps:

. *Check AWS CLI version*: make sure you have the latest version installed.
+
[source,bash]
----
aws --version
----

. *Export the AWS profile*:
+
[source,bash]
----
export AWS_PROFILE=<profile-name>
----

. *Assume the role and save credentials*.
+
[source,bash]
----
aws sts assume-role \
  --role-arn arn:aws:iam::<accountID>:role/<role-name> \
  --role-session-name eks-session > creds.json
----

. *Export the temporary credentials*.
+
[source,bash]
----
export AWS_ACCESS_KEY_ID=$(jq -r '.Credentials.AccessKeyId' creds.json)
export AWS_SECRET_ACCESS_KEY=$(jq -r '.Credentials.SecretAccessKey' creds.json)
export AWS_SESSION_TOKEN=$(jq -r '.Credentials.SessionToken' creds.json)
----

. *Update the _kubeconfig_*.
+
[source,bash]
----
aws eks update-kubeconfig --region <region> --name <cluster-name>
----

== Infrastructure operation

image::controllers.png[]

_Stratio KEOS_ allows multiple advanced operations to be performed by interacting with the _Stratio Cluster Operator_ (infrastructure as code or IaC), which in its reconciliation cycle in turn interacts with the different providers to perform the requested operations.

=== Self-healing

image::self-healing.png[]

The self-healing capability of the cluster is managed by the _MachineHealthCheck_ object:

[source,bash]
----
$ kubectl -n cluster-example get mhc -o yaml
...
  spec:
    clusterName: example
    maxUnhealthy: 100%
    nodeStartupTimeout: 5m0s
    selector:
      matchLabels:
        keos.stratio.com/machine-role: example-worker-node
    unhealthyConditions:
    - status: Unknown
      timeout: 1m0s
      type: Ready
    - status: "False"
      timeout: 1m0s
      type: Ready
...
----

NOTE: In Unmanaged Azure, there will be a _MachineHealthCheck_ for the _control-plane_ and another for the _worker_ nodes, while managed ones (EKS, GKE) will only have the second one.

==== Failover test on a node

In case of failure in a node, it will be detected by a controller and it will be replaced by deleting it and recreating another one of the same group, which ensures the same characteristics.

To simulate a VM failure, it will be deleted from the cloud provider's web console.

The recovery of the node comprises the following phases and estimated times (which may vary depending on the provider and the flavour):

[source,bash]
----
. Terminate VM from console: 0s
. New VM is Provisioning: 50s
. Old Machine is Deleted & the new one is Provisioned: 1m5s
. New Machine is Running & new k8s node is NotReady: 1m 50s
. New k8s node is Ready: 2m
----

=== Static scaling

Although manual scaling of an existing node group is discouraged, these operations are provided for cases without autoscaling or new node groups.

==== Scaling a _workers_ group

image::escalado-manual.png[]

To manually scale a group of _workers_ the _KeosCluster_ object is used:

[source,bash]
----
kubectl -n cluster-example-eks edit keoscluster
----

[source,yaml]
----
spec:
  ...
  worker_nodes:
    - name: eks-prod-xlarge
      quantity: 9
      ...
----

Verify the change by querying the state of the _KeosCluster_ object:

[source,bash]
----
kubectl -n cluster-<cluster_name> get keoscluster <cluster_name> --subresource=status
----

==== Create a new workers group

To create a new group of nodes just create a new element to the array _worker++_++nodes_ of the _KeosCluster_ object:

[source,yaml]
----
spec:
  ...
  worker_nodes:
    - ...
    - name: eks-prod-xlarge
      quantity: 6
      max_size: 18
      min_size: 6
      size: m6i.xlarge
      labels:
        disktype: standard
      root_volume:
        size: 50
        type: gp3
        encrypted: true
      ssh_key: stg-key
----

Again, verify the change by querying the state of the _KeosCluster_ object:

[source,bash]
----
kubectl -n cluster-<cluster_name> get keoscluster <cluster_name> --subresource=status
----

==== Vertical scaling

The vertical scaling of a node group is done by modifying the instance type in the _KeosCluster_ object corresponding to the group.

[source,yaml]
----
spec:
  ...
  worker_nodes:
    - name: eks-prod-xlarge
      size: m6i.2xlarge
      ...
----

=== Autoscaling

image::autoescalado.png[]

For node autoscaling, _cluster-autoscaler_ is used, which will detect pods pending execution due to lack of resources and will scale groups of nodes according to the deployment filters.

This operation is performed in the API Server, being the controllers in charge of creating the VMs in the cloud provider and adding them to the cluster as Kubernetes _worker_ nodes.

Since the autoscaling is based on the _cluster-autoscaler_, the minimum and maximum will be added in the node group in the _KeosCluster_ object:

[source,yaml]
----
spec:
  ...
  worker_nodes:
    - name: eks-prod-xlarge
      min_size: 6
      max_size: 21
      ...
----

==== Test

To test autoscaling, you can create a deployment with enough replicas to prevent them from running on the current nodes:

[source,bash]
----
kubectl create deploy test --replicas 1500 --image nginx:alpine
----

At the end of the test, remove the deployment:

[source,bash]
----
kubectl --kubeconfig demo-eks.kubeconfig delete deploy test
----

==== Logs

The logs of the _cluster-autoscaler_ can be viewed from its deployment:

[source,bash]
----
kubectl -n kube-system logs -f -l app.kubernetes.io/name=clusterapi-cluster-autoscaler
----

=== Kubernetes upgrade

The upgrade of the cluster to a higher version of Kubernetes will be performed in two parts within the same atomic process: first the _control-plane_ and, once this is on the new version, the _worker_ nodes, iterating through each group and upgrading them one by one.

CAUTION: Upgrading the Kubernetes version of nodes in clusters where the image has not been specified may involve an OS upgrade.

image::upgrade-cp.png[]

image::upgrade-w.png[]

[CAUTION]
====
A misconfigured Pod Disruption Budget (PDB) can block the removal of a pod. This happens if the PDB requires at least one available replica, but the resource has only one deployed. In this case, the replica cannot be removed, preventing the node from draining, which may affect updates.

To avoid this issue:

. Ensure that deployments have more than one replica if the PDB requires at least one available.
. Before updating the cluster, review this configuration to prevent blockages.
. If the resource has only one replica, you can temporarily remove the PDB to allow the update.
. Before upgrading the cluster, check the PDBs to avoid potential blocking issues.
. If a resource has only one replica, you can temporarily delete the PDB to allow the upgrade.

In EKS, for example, it is recommended to check if the `coredns` PDB exists in the `kube-system` namespace and delete it before upgrading the cluster:

[source,bash]
----
kubectl -n kube-system get poddisruptionbudget coredns
kubectl -n kube-system delete poddisruptionbudget coredns
----
====

==== Prerequisites

The version upgrade of a cluster in productive environments and especially in unmanaged flavours must be done with extreme caution. In particular, before upgrading it is recommended to do a backup of the objects that manage the infrastructure with the following command:

[source,bash]
----
clusterctl --kubeconfig ./kubeconfig/path move -n cluster-<cluster_name> --to-directory ./backup/path/
----

In the case of a managed _control-plane_, it should be verified that the desired version of Kubernetes is supported by the provider.

===== EKS

Prior to upgrading EKS you must make sure that the desired version is supported. To do this you can use the following command:

[source,bash]
----
aws eks describe-addon-versions | jq -r ".addons[] | .addonVersions[] | .compatibilities[] | .clusterVersion" | sort -nr | uniq | head -4
----

===== Azure unmanaged

The _GlobalNetworkPolicy_ created for the _control-plane_ in the _Stratio KEOS_ installation phase should be modified so that it *permits all node networking momentarily* while the version upgrade is running.

Once completed, the internal IPs of the nodes and the tunnel IPs assigned to those nodes should be updated:

[source,bash]
----
kubectl get nodes -l node-role.kubernetes.io/control-plane= -ojson | jq -r '.items[].status.addresses[] | select(.type=="InternalIP").address + "\/32"'
----

[source,bash]
----
IPAMHANDLERS=$(kw get ipamhandles -oname | grep control-plane)
for handler in $IPAMHANDLERS; do kw get $handler -o json | jq -r '.spec.block | keys[]' | sed 's/\/.*/\/32/'; done
----

==== Initiate the upgrade

To initiate the upgrade, once the prerequisites are satisfied a patch of _spec.k8s++_++version_ will be run on the _KeosCluster_ object:

[source,bash]
----
kubectl -n cluster-<cluster_name> patch KeosCluster <cluster_name> --type merge -p '{"spec": {"k8s_version": "v1.28.1"}}'
----

NOTE: The controller provisions a new node from the _workers_ cluster with the updated version and, once it is _Ready_ in Kubernetes, removes a node with the old version. In this way, it always ensures the configured number of nodes.

==== Checking etcd

One way to ensure that etcd is correct after updating an unmanaged _control-plane_ is to open a terminal on any pod of etcd, view the cluster status, and compare the IPs of the registered members with those of the _control-plane_ nodes.

[source,bash]
----
k -n kube-system exec -ti etcd-<control-plane-node> sh

alias e="etcdctl --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --cacert=/etc/kubernetes/pki/etcd/ca.crt "
e endpoint status
e endpoint status -w table --cluster
e member list
e member remove <member-id>
----

=== Cluster removal

[NOTE]
.Preliminary considerations
====
Before deleting the cloud provider resources generated by _Stratio Cloud Provisioner_ you must delete those that have been created by the _keos-installer_ or any external automatism (for example, the _Services_ of type _LoadBalancer_).

Also, you should note that the process requires the _clusterctl_ binary on the bastion machine (any computer with access to the _API Server_) on which it will run.
====

Run the following steps to perform the cluster removal:

. Create a local cluster indicating that no object is generated in the cloud provider.
+
[source,bash]
----
[bastion]$ sudo ./bin/cloud-provisioner create cluster --name <cluster_name> --descriptor cluster.yaml --vault-password <my-passphrase> --avoid-creation
----

. Pause the controller of the _Stratio Cluster Operator_:
+
[source,bash]
----
[bastion]$ kubectl --kubeconfig $KUBECONFIG -n kube-system scale deployment keoscluster-controller-manager --replicas 0
----

. Move the cluster _worker_ management to the local cluster using the corresponding _kubeconfig_ (note that for managed _control-planes_ the _kubeconfig_ of the provider will be needed). To ensure this step, look for the following text in the command output: "Moving Cluster API objects Clusters=1".
+
[source,bash]
----
[bastion]$ sudo clusterctl --kubeconfig $KUBECONFIG move -n cluster-<cluster_name> --to-kubeconfig /root/.kube/config
----

. Access the local cluster and delete the cluster _worker_.
+
[source,bash]
----
[bastion]$ sudo docker exec -ti <nombre_cluster>-control-plane bash
root@<nombre_del_cluster>-control-plane:/# kubectl -n cluster-<nombre_del_cluster> delete cl --all
cluster.cluster.x-k8s.io "<nombre_del_cluster>" eliminado
root@<nombre_del_cluster>-plano-de-control:/#
----

. Finally, remove the local cluster.
+
[source,bash]
----
[bastion]$ sudo ./bin/cloud-provisioner delete cluster --name <nombre_cluster>
----

== Offline installation

To learn how to perform an installation where the workloads images of the cluster come from repositories accessible from environments without internet access, see the xref:operations-manual:offline-installation.adoc[offline installation manual].

== Credential management

To manage the credentials configured in the cluster, please look at the xref:operations-manual:credentials.adoc[credential management documentation].
