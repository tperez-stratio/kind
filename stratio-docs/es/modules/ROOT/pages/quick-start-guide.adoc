= GuÃ­a de inicio rÃ¡pido

== EKS

=== Prerrequisitos

* Usuario con los privilegios necesarios en AWS:
** Crear el usuario para la instalaciÃ³n.
** Crear la polÃ­tica segÃºn xref:attachment$stratio-eks-policy.json[_stratio-eks-policy.json_].
** Crear la polÃ­tica segÃºn xref:attachment$stratio-aws-temp-policy.json[_stratio-temp-policy.json_] (sÃ³lo para el aprovisionamiento).
** Adjuntar polÃ­ticas al usuario.
** Crear una clave de acceso.
* Zonas DNS privadas y pÃºblicas creadas en AWS (opcional).
* Infraestructura personalizada creada en AWS (opcional).
* Componer el fichero descriptor del _cluster_.
** Credenciales del usuario (_access++_++key_ y _secret++_++key_) y datos de la cuenta (regiÃ³n y _account++_++id_), que se cifrarÃ¡n en la primera ejecuciÃ³n.
** Token de GitHub para descargar plantillas (opcional).
** Datos de la cuenta (regiÃ³n y _account++_++id_).
** Datos de la infraestructura ya creada (opcional).
** GestiÃ³n de las zonas DNS creadas (opcional).
** URL de ECR.
** Dominio externo del _cluster_.
** Habilitar el _logging_ en EKS por componente (opcional).
** Grupos de nodos.
** InformaciÃ³n necesaria para la instalaciÃ³n de _Stratio KEOS_.

En cuanto al _control-plane_, en el descriptor del _cluster_ se puede indicar que se trata de un *_control-plane_ gestionado* y los _logs_ que se quieren activar del mismo (_APIserver_, _audit_, _authenticator_, _controller++_++manager_ y/o _scheduler_).

Asimismo, se pueden indicar *grupos de nodos _worker_* con las siguientes opciones:

* _name_: nombre del grupo, no puede repetirse.
* _size_: tipo de instancia.
* _quantity_: cantidad de _workers_ en el grupo.
* _min++_++size_: nÃºmero mÃ­nimo de nodos para el autoescalado (opcional).
* _max++_++size_: nÃºmero mÃ¡ximo de nodos para el autoescalado (opcional).
* _labels_: etiquetas de los nodos en Kubernetes (opcional).
* _root++_++volume_: particularidades del disco (opcional).
** _size_: tamaÃ±o en GB (por defecto: 30GB).
** _type_: tipo de disco (por defecto: gp3).
** _encrypted_: cifrado del disco (por defecto: _false_).
* _ssh++_++key_: clave SSH para acceso a los nodos (opcional). Debe existir en el proveedor.
* _spot_: indica si la instancia es de tipo _spot_ (opcional).
* _node++_++image_: imagen de los nodos _worker_ (opcional). La imagen indicada deberÃ¡ existir y ser compatible con EKS.
* _zone++_++distribution_: indica si la cantidad de nodos debe quedar balanceada en las zonas o no (por defecto: _balanced_).
* _az_: zona del grupo de _workers_ (opcional). En caso de especificarse, solamente se utilizarÃ¡ Ã©sta para todo el grupo. Este parÃ¡metro invalida lo especificado en _zone++_++distribution_.

NOTE: Por defecto, el reparto de nodos se harÃ¡ en las zonas a, b y c de la regiÃ³n indicada de forma balanceada, por lo tanto, el resto de la divisiÃ³n por tres de la cantidad de nodos se descartarÃ¡. Ejemplo: si se indica "quantity=7", sÃ³lo se desplegarÃ¡n 2 nodos en cada una de las zonas.

==== _keos-installer_

A modo de facilitar la instalaciÃ³n de _Stratio KEOS_, en el proceso de provisiÃ³n se genera un fichero _keos.yaml_ funcional y listo para poder lanzar la instalaciÃ³n. Para ello, en el descriptor del _cluster_ se podrÃ¡ indicar la versiÃ³n y _flavour_ (_production_, _development_ o _minimal_).

[source,yaml]
----
  keos:
    version: 1.0.2
    flavour: development
----

Para cualquier personalizaciÃ³n extra, deberÃ¡ modificarse el fichero antes de ejecutar el _keos-installer_.

==== Consideraciones

* En caso de utilizar infraestructura personalizada, se deberÃ¡ indicar la VPC y 3 _subnets_, uno por zona de la regiÃ³n (a, b y c).
* La versiÃ³n de Kubernetes indicada debe estar soportada por EKS.
* Los nombres de los grupos de _worker++_++nodes_ no pueden repetirse.

TIP: Para mÃ¡s detalles, consulta la xref:ROOT:installation.adoc[guÃ­a de instalaciÃ³n].

=== InstalaciÃ³n

Esta fase (aprovisionamiento e instalaciÃ³n de Kubernetes) deberÃ¡ ejecutarse desde una mÃ¡quina Linux con acceso a internet y un Docker instalado.

Una vez descargado el fichero `.tgz` del _cloud-provisioner_, se procederÃ¡ a descomprimirlo y ejecutarlo con los parÃ¡metros de creaciÃ³n:

[source,console]
----
$ tar xvzf cloud-provisioner-*tar.gz
$ sudo ./bin/cloud-provisioner create cluster --name <cluster_id> --descriptor cluster.yaml
Vault Password:
Creating temporary cluster "example-eks" ...
 âœ“ Ensuring node image (kindest/node:v1.27.0) ğŸ–¼
 âœ“ Building Stratio image (stratio-capi-image:v1.27.0) ğŸ“¸
 âœ“ Preparing nodes ğŸ“¦
 âœ“ Writing configuration ğŸ“œ
 âœ“ Starting control-plane ğŸ•¹ï¸
 âœ“ Installing CNI ğŸ”Œ
 âœ“ Installing StorageClass ğŸ’¾
 âœ“ Installing CAPx ğŸ–ï¸
 âœ“ Generating secrets file ğŸ“ğŸ—ï¸
 âœ“ Installing keos cluster operator ğŸ’»
 âœ“ Creating the workload cluster ğŸ’¥
 âœ“ Saving the workload cluster kubeconfig ğŸ“
 âœ“ Preparing nodes in workload cluster ğŸ“¦
 âœ“ Installing AWS LB controller in workload cluster âš–ï¸
 âœ“ Installing StorageClass in workload cluster ğŸ’¾
 âœ“ Enabling workload clusters self-healing ğŸ¥
 âœ“ Installing CAPx in workload cluster ğŸ–ï¸
 âœ“ Configuring Network Policy Engine in workload cluster ğŸš§
 âœ“ Installing cluster-autoscaler in workload cluster ğŸ—š
 âœ“ Installing keos cluster operator in workload cluster ğŸ’»
 âœ“ Creating cloud-provisioner Objects backup ğŸ—„ï¸
 âœ“ Moving the management role ğŸ—ï¸
 âœ“ Executing post-install steps ğŸ–ï¸
 âœ“ Generating the KEOS descriptor ğŸ“
 âœ“ Rotating and generating override_vars structure âš’ï¸

The cluster has been installed successfully. Please refer to the documents below on how to proceed:
1. Post-installation Stratio cloud-provisioner documentation
2. Stratio KEOS documentation
----

=== Siguientes pasos

En este punto, habrÃ¡ un _cluster_ de Kubernetes con las caracterÃ­sticas indicadas en el descriptor y se podrÃ¡ acceder al _API Server_ de EKS con el CLI de AWS como lo indica en https://docs.aws.amazon.com/eks/latest/userguide/create-kubeconfig.html[la documentaciÃ³n oficial].

[source,console]
----
aws eks update-kubeconfig --region <region> --name <cluster_id> --kubeconfig ./<cluster_id>.kubeconfig

kubectl --kubeconfig ./<cluster_id>.kubeconfig get nodes
----

AquÃ­, se podrÃ¡n eliminar los permisos de _clusterawsadm.json_.

A continuaciÃ³n, se procederÃ¡ a desplegar _Stratio KEOS_ *utilizando _keos-installer_*.

== GKE

=== Prerrequisitos

* Habilitar la API de Kubernetes Engine en GCP.
* Usuario con los privilegios necesarios en GCP:
** Crear una _IAM Service Account_ con los permisos definidos en:
*** xref:attachment$stratio-gcp-permissions.list[stratio-gcp-permissions.list].
*** xref:attachment$stratio-gke-permissions.list[stratio-gke-permissions.list].
** Crear una clave privada para la _IAM Service Account_ de tipo JSON y descargarla en un fichero `<project_name>-<id>.json`. Estos datos se utilizarÃ¡n para las credenciales solicitadas en el descriptor del _cluster_.
* Zonas DNS privadas y pÃºblicas creadas en GCP (opcional).
* Infraestructura personalizada creada en GCP (opcional).
* Componer el fichero descriptor del _cluster_.
** Credenciales del usuario (_private++_++key++_++id_, _private++_++key_ y _client++_++email_) y datos de la cuenta (regiÃ³n y _project++_++id_), que se cifrarÃ¡n en la primera ejecuciÃ³n.
** _Token_ de GitHub para la descarga de plantillas (opcional).
** Datos de la infraestructura ya creada (opcional).
** GestiÃ³n de las zonas DNS creadas (opcional).
** Datos del _Docker registry_ (URL, credenciales).
** Dominio externo del _cluster_.
** _control-plane_.
** Grupos de nodos.
** InformaciÃ³n necesaria para la instalaciÃ³n de _Stratio KEOS_.

NOTE: La instalaciÃ³n *no requiere* una imagen personalizada.

TIP: Se recomienda crear un bastiÃ³n para proceder a la instalaciÃ³n.

==== Requisitos del bastiÃ³n

- Tener Docker instalado (versiÃ³n 27.0.3 o superior).
- Tener una imagen local: _stratio-capi-image:v1.27.0_.

==== _control-plane_

En cuanto al _control-plane_, en el descriptor del _cluster_ se puede indicar que se trata de un *_control-plane_ gestionado* y se deben incluir las siguientes especificaciones:

* _cluster++_++network_ (obligatorio): define la red del _cluster_.
** _private++_++cluster_ (obligatorio): define la _spec_ del _cluster_ privado.
*** _enable++_++private++_++endpoint_ (obligatorio/immutable; por defecto: "true"): indica si se utiliza la direcciÃ³n IP interna del _master_ como _endpoint_ del _cluster_.
*** _control++_++plane++_++cidr++_++block_ (master-ipv4-cidr) (obligatorio/immutable): es el rango de IP en notaciÃ³n CIDR que se utilizarÃ¡ para la red del _master_. Este rango no debe superponerse con ningÃºn otro rango en uso dentro de la red del _cluster_. Se aplica cuando _enabled++_++private++_++nodes_ es "true" (valor por defecto) y debe ser una subred /28.
* _ip++_++allocation++_++policy_ (opcional/immutable): representa las opciones de configuraciÃ³n para la asignaciÃ³n de IP del _cluster_ GKE (si no se especifica, se usarÃ¡n los valores predeterminados de GKE).
** _cluster++_++ipv4++_++cidr++_++block_: representa el rango de direcciones IP para las IP de los _pods_ del _cluster_ GKE (si no se especifica, se elegirÃ¡ el rango con el tamaÃ±o predeterminado).
** _services++_++ipv4++_++cidr++_++block_: representa el rango de direcciones IP para las IP de los servicios del _cluster_ GKE (si no se especifica, se elegirÃ¡ el rango con el tamaÃ±o predeterminado).
** _cluster++_++secondary++_++range++_++name_: representa el nombre del rango secundario que se utilizarÃ¡ para el bloque CIDR del _cluster_ GKE. El rango se utilizarÃ¡ para las direcciones IP de los _pods_ y debe ser un rango secundario existente asociado con la subred del _cluster_.
** _services++_++secondary++_++range++_++name_: representa el nombre del rango secundario que se utilizarÃ¡ para el bloque CIDR de los servicios. El rango se utilizarÃ¡ para las IP de los servicios y debe ser un rango secundario existente asociado con la subred del _cluster_.

NOTE: Si los rangos de IP ya estÃ¡n creados, se deben usar los nombres especificados (_services++_++secondary++_++range++_++name_ y _cluster++_++secondary++_++range++_++name_). Si no existen, se debe usar la notaciÃ³n CIDR (_services++_++ipv4++_++cidr++_++block_ y _cluster++_++ipv4++_++cidr++_++block_) para crearlos, pero no se pueden usar ambos mÃ©todos simultÃ¡neamente.

* _master++_++authorized++_++networks++_++config_ (opcional/immutable): representa la configuraciÃ³n de redes autorizadas del _cluster_.
** _cidr++_++blocks_ (opcional, ya que _gcp++_++public++_++cidrs++_++access++_++enabled_ es siempre "true"): lista de bloques CIDR que se permiten acceder al _master_.
*** _cidr++_++block_: rango de IP en notaciÃ³n CIDR que se permitirÃ¡ acceder al _master_.
*** _display++_++name_: nombre de la red autorizada.
** _gcp++_++public++_++cidrs++_++access++_++enabled_ (por defecto: "false"), si _enable++_++private++_++endpoint_ es "true"): indica si se permite el acceso a las direcciones IP pÃºblicas de Google Compute Engine.

NOTE: Al habilitar la configuraciÃ³n de redes autorizadas, se impedirÃ¡ que todo el trÃ¡fico externo acceda al _master_ de Kubernetes a travÃ©s de HTTPS excepto el trÃ¡fico de los bloques de CIDR especificados, las IP pÃºblicas de Google Compute Engine y las IP de los servicios de Google Cloud.

* _monitoring++_++config_ (opcional/immutable): define la monitorizaciÃ³n del _cluster_.
** _enable++_++managed++_++prometheus_ (por defecto: "false"): habilita la monitorizaciÃ³n gestionada del _cluster_ con Prometheus.
* _logging++_++config_ (opcional/immutable): define la configuraciÃ³n de _logging_ del _cluster_.
** _system++_++components_ (por defecto: "false"): habilita el componente _system_ de _logging_.
** _workloads_ (por defecto: "false"): habilita el componente _workloads_ de _logging_.

NOTE: Cualquier modificaciÃ³n de los parÃ¡metros anteriormente mencionados no tendrÃ¡ efecto, sÃ³lo se aplican en tiempo de creaciÃ³n del _cluster_.

==== Nodos _worker_

En el descriptor del _cluster_ se pueden indicar grupos de nodos _worker_ con las siguientes opciones:

* _name_: nombre del grupo, no puede repetirse.
* _size_: tipo de instancia.
* _quantity_: cantidad de _workers_ en el grupo.
* _min++_++size_: nÃºmero mÃ­nimo de nodos para el autoescalado (opcional).
* _max++_++size_: nÃºmero mÃ¡ximo de nodos para el autoescalado (opcional).
* _labels_: etiquetas de los nodos en Kubernetes (opcional).
* _taints_: _taints_ de los nodos en Kubernetes (opcional).
* _root++_++volume_: particularidades del disco (opcional).
** _size_: tamaÃ±o en GB (por defecto: 30GB).
** _type_: tipo de disco (por defecto: Managed).
* _zone++_++distribution_: indica si la cantidad de nodos debe quedar balanceada en las zonas o no (por defecto: _balanced_).
* _az_: zona del grupo de _workers_ (opcional). En caso de especificarse, solamente se utilizarÃ¡ esta para todo el grupo. Este parÃ¡metro invalida lo especificado en _zone++_++distribution_.

NOTE: Por defecto, el reparto de nodos se harÃ¡ en las zonas a, b y c de la regiÃ³n indicada de forma balanceada, por lo tanto, el resto de la divisiÃ³n por tres de la cantidad de nodos se descartarÃ¡. Ejemplo: si se indica 'quantity=7', sÃ³lo se desplegarÃ¡n 2 nodos en cada una de las zonas.

==== _keos-installer_

A modo de facilitar la instalaciÃ³n de _Stratio KEOS_, en el proceso de provisiÃ³n se genera un fichero _keos.yaml_ funcional y listo para poder lanzar la instalaciÃ³n. Para ello, en el descriptor del _cluster_ se podrÃ¡ indicar la versiÃ³n y _flavour_ (_production_, _development_ o _minimal_).

[source,yaml]
----
  keos:
    version: 1.1.2
    flavour: development
----

Para cualquier personalizaciÃ³n extra, deberÃ¡ modificarse el fichero antes de ejecutar el _keos-installer_.

==== Consideraciones

* En caso de utilizar una infraestructura personalizada, se deberÃ¡ indicar la VPC y la subred de la regiÃ³n.
+
[source,yaml]
----
  networks:
    vpc_id: "vpc-name"
    subnets:
      - subnet_id: "subnet-name"
----

* La versiÃ³n de Kubernetes debe ser (1.28) y estar soportada por GKE.
* Los nombres de los grupos de _worker++_++nodes_ no pueden repetirse.

TIP: Para mÃ¡s detalles, consulta la xref:ROOT:installation.adoc[guÃ­a de instalaciÃ³n].

=== InstalaciÃ³n

Esta fase (aprovisionamiento e instalaciÃ³n de Kubernetes) deberÃ¡ ejecutarse desde la mÃ¡quina bastiÃ³n.

Una vez descargado el fichero `.tgz` del _cloud-provisioner_, se procederÃ¡ a descomprimirlo y ejecutarlo con los parÃ¡metros de creaciÃ³n:

[source,console]
----
$ tar xvzf cloud-provisioner-*tar.gz
$ sudo ./bin/cloud-provisioner create cluster --name <cluster_id> --use-local-stratio-image --descriptor cluster.yaml
Vault Password:
Creating temporary cluster "example-gke" ...
 âœ“ Using local Stratio image (stratio-capi-image:v1.27.0) ğŸ–¼
 âœ“ Preparing nodes ğŸ“¦
 âœ“ Writing configuration ğŸ“œ
 âœ“ Starting control-plane ğŸ•¹ï¸
 âœ“ Installing StorageClass ğŸ’¾
 âœ“ Installing Private CNI ğŸ–ï¸
 âœ“ Deleting local storage plugin ğŸ–ï¸
 âœ“ Installing CAPx ğŸ–ï¸
 âœ“ Generating secrets file ğŸ“ğŸ—ï¸
 âœ“ Installing keos cluster operator ğŸ’»
 âœ“ Creating the workload cluster ğŸ’¥
 âœ“ Saving the workload cluster kubeconfig ğŸ“
 âœ“ Preparing nodes in workload cluster ğŸ“¦
 âœ“ Enabling CoreDNS as DNS server ğŸ“¡
 âœ“ Installing CAPx in workload cluster ğŸ–ï¸
 âœ“ Installing StorageClass in workload cluster ğŸ’¾
 âœ“ Enabling workload cluster's self-healing ğŸ¥
 âœ“ Configuring Network Policy Engine in workload cluster ğŸš§
 âœ“ Installing keos cluster operator in workload cluster ğŸ’»
 âœ“ Creating cloud-provisioner Objects backup ğŸ—„ï¸
 âœ“ Moving the management role ğŸ—ï¸
 âœ“ Executing post-install steps ğŸ–ï¸
 âœ“ Generating the KEOS descriptor ğŸ“
 âœ“ Rotating and generating override_vars structure âš’ï¸
The cluster has been installed successfully. Please refer to the documents below on how to proceed:
1. Post-installation Stratio cloud-provisioner documentation.
2. Stratio KEOS documentation.
----

=== Siguientes pasos

En este punto, habrÃ¡ un _cluster_ de Kubernetes con las caracterÃ­sticas indicadas en el descriptor y se podrÃ¡ acceder al _API Server_ con el _kubeconfig_ generado en el directorio actual (_.kube/config_):

[source,console]
----
kubectl --kubeconfig .kube/config get nodes
----

A continuaciÃ³n, se procederÃ¡ a desplegar _Stratio KEOS_ *utilizando _keos-installer_*.

== Azure no gestionado

=== Prerrequisitos

* Usuarios con los privilegios necesarios en Azure:
** Crear una _Managed Identity_ con los roles: _Contributor_, _AcrPull_ (sobre el ACR del _cluster_, opcional) y _Managed Identity Operator_. La referencia de esta identidad (_Resource ID_) se utilizarÃ¡ en el descriptor del _cluster_ (formato _/subscriptions/<subscription_id>/resourcegroups/<nombre_resourcegroup>/providers/Microsoft.ManagedIdentity/userAssignedIdentities/<identity_name>_).
** Crear una _App registration_ (crearÃ¡ una _Enterprise application_) y generar un _client secret_. El valor del _client secret_ y su _Secret ID_ se utilizarÃ¡n para las credenciales solicitadas en el descriptor del _cluster_.
* Zonas DNS privadas y pÃºblicas creadas en Azure (opcional).
* Infraestructura personalizada creada en Azure (opcional).
* Componer el fichero descriptor del _cluster_.
** Credenciales del usuario (_client++_++id_ y _client++_++secret_) y datos de la cuenta (_subscription++_++id_ y _tenant++_++id_), que se cifrarÃ¡n en la primera ejecuciÃ³n.
** _Token_ de GitHub para la descarga de plantillas (opcional).
** Datos de la infraestructura ya creada (opcional).
** GestiÃ³n de las zonas DNS creadas (opcional).
** Datos del _Docker registry_ (URL, credenciales).
** Dominio externo del _cluster_.
** _control-plane_.
** Grupos de nodos.
** InformaciÃ³n necesaria para la instalaciÃ³n de _Stratio KEOS_.

NOTE: La instalaciÃ³n requiere una imagen personalizada de Ubuntu 22.04 con parÃ¡metros necesarios para Elasticsearch.

==== Nodos _control-plane_

Para este proveedor, el _control-plane_ se desplegarÃ¡ en mÃ¡quinas virtuales, por ello, se podrÃ¡n configurar las siguientes opciones:

* _highly++_++available_: define si el _control-plane_ contarÃ¡ con alta disponibilidad (por defecto: _true_).
* _managed_: indica que se trata de un _control-plane_ en mÃ¡quinas virtuales.
* _size_: tipo de instancia.
* _node++_++image_: imagen de los nodos del _control-plane_ (opcional). La imagen indicada deberÃ¡ existir en la cuenta.
* _root++_++volume_: particularidades del disco (opcional).
** _size_: tamaÃ±o en GB (por defecto: 30GB).
** _type_: tipo de disco (por defecto: Standard_LRS).

==== Nodos _worker_

En el descriptor del _cluster_ se pueden indicar grupos de nodos _worker_ con las siguientes opciones:

* _name_: nombre del grupo, no puede repetirse.
* _size_: tipo de instancia.
* _quantity_: cantidad de _workers_ en el grupo.
* _min++_++size_: nÃºmero mÃ­nimo de nodos para el autoescalado (opcional).
* _max++_++size_: nÃºmero mÃ¡ximo de nodos para el autoescalado (opcional).
* _labels_: etiquetas de los nodos en Kubernetes (opcional).
* _root++_++volume_: particularidades del disco (opcional).
** _size_: tamaÃ±o en GB (por defecto: 30GB).
** _type_: tipo de disco (por defecto: Standard_LRS).
* _ssh++_++key_: clave SSH para acceso a los nodos (opcional). Debe existir en el proveedor.
* _spot_: indica si la instancia es de tipo _spot_ (opcional).
* _node++_++image_: imagen de los nodos _worker_ (opcional). La imagen indicada deberÃ¡ existir en la cuenta.
* _zone++_++distribution_: indica si la cantidad de nodos debe quedar balanceada en las zonas o no (por defecto: _balanced_).
* _az_: zona del grupo de _workers_ (opcional). En caso de especificarse, solamente se utilizarÃ¡ Ã©sta para todo el grupo. Este parÃ¡metro invalida lo especificado en _zone++_++distribution_.

NOTE: Por defecto, el reparto de nodos se harÃ¡ en las zonas a, b y c de la regiÃ³n indicada de forma balanceada, por lo tanto, el resto de la divisiÃ³n por tres de la cantidad de nodos se descartarÃ¡. Ejemplo: si se indica 'quantity=7', sÃ³lo se desplegarÃ¡n 2 nodos en cada una de las zonas.

==== _keos-installer_

A modo de facilitar la instalaciÃ³n de _Stratio KEOS_, en el proceso de provisiÃ³n se genera un fichero _keos.yaml_ funcional y listo para poder lanzar la instalaciÃ³n. Para ello, en el descriptor del _cluster_ se podrÃ¡ indicar la versiÃ³n y _flavour_ (_production_, _development_ o _minimal_).

[source,yaml]
----
  keos:
    version: 1.0.2
    flavour: development
----

Para cualquier personalizaciÃ³n extra, deberÃ¡ modificarse el fichero antes de ejecutar el _keos-installer_.

==== Consideraciones

* En caso de utilizar una infraestructura personalizada, se deberÃ¡ indicar la VPC y 3 _subnets_, uno por zona de la regiÃ³n (a, b y c).
* La versiÃ³n de Kubernetes configurada debe ser la soportada en las imÃ¡genes indicadas (opcional).
* Los nombres de los grupos de _worker++_++nodes_ no pueden repetirse.

TIP: Para mÃ¡s detalles, consulta la xref:ROOT:installation.adoc[guÃ­a de instalaciÃ³n].

=== InstalaciÃ³n

Esta fase (aprovisionamiento e instalaciÃ³n de Kubernetes) deberÃ¡ ejecutarse desde una mÃ¡quina Linux con acceso a internet y un Docker instalado.

Una vez descargado el fichero `.tgz` del _cloud-provisioner_, se procederÃ¡ a descomprimirlo y ejecutarlo con los parÃ¡metros de creaciÃ³n:

[source,console]
----
$ tar xvzf cloud-provisioner-*tar.gz
$ sudo ./bin/cloud-provisioner create cluster --name <cluster_id> --descriptor cluster.yaml
Vault Password:
Creating temporary cluster "example-azure" ...
 âœ“ Ensuring node image (kindest/node:v1.27.0) ğŸ–¼
 âœ“ Building Stratio image (stratio-capi-image:v1.27.0) ğŸ“¸
 âœ“ Preparing nodes ğŸ“¦
 âœ“ Writing configuration ğŸ“œ
 âœ“ Starting control-plane ğŸ•¹ï¸
 âœ“ Installing CNI ğŸ”Œ
 âœ“ Installing StorageClass ğŸ’¾
 âœ“ Installing CAPx ğŸ–ï¸
 âœ“ Generating secrets file ğŸ“ğŸ—ï¸
 âœ“ Installing keos cluster operator ğŸ’»
 âœ“ Creating the workload cluster ğŸ’¥
 âœ“ Saving the workload cluster kubeconfig ğŸ“
 âœ“ Installing cloud-provider in workload cluster â˜ï¸
 âœ“ Installing Calico in workload cluster ğŸ”Œ
 âœ“ Installing CSI in workload cluster ğŸ’¾
 âœ“ Preparing nodes in workload cluster ğŸ“¦
 âœ“ Installing StorageClass in workload cluster ğŸ’¾
 âœ“ Enabling workload clusters self-healing ğŸ¥
 âœ“ Installing CAPx in workload cluster ğŸ–ï¸
 âœ“ Installing cluster-autoscaler in workload cluster ğŸ—š
 âœ“ Installing keos cluster operator in workload cluster ğŸ’»
 âœ“ Creating cloud-provisioner Objects backup ğŸ—„ï¸
 âœ“ Moving the management role ğŸ—ï¸
 âœ“ Executing post-install steps ğŸ–ï¸
 âœ“ Generating the KEOS descriptor ğŸ“

The cluster has been installed successfully. Please refer to the documents below on how to proceed:
1. Post-installation Stratio cloud-provisioner documentation
2. Stratio KEOS documentation
----

=== Siguientes pasos

En este punto, habrÃ¡ un _cluster_ de Kubernetes con las caracterÃ­sticas indicadas en el descriptor y se podrÃ¡ acceder al _API Server_ con el _kubeconfig_ generado en el directorio actual (_.kube/config_):

[source,console]
----
kubectl --kubeconfig .kube/config get nodes
----

A continuaciÃ³n, se procederÃ¡ a desplegar _Stratio KEOS_ *utilizando _keos-installer_*.

=== Siguientes pasos

En este punto, habrÃ¡ un _cluster_ de Kubernetes con las caracterÃ­sticas indicadas en el descriptor y se podrÃ¡ acceder al _API Server_ con el _kubeconfig_ generado en el directorio actual (_.kube/config_):

[source,console]
----
kubectl --kubeconfig .kube/config get nodes
----

A continuaciÃ³n, se procederÃ¡ a desplegar _Stratio KEOS_ *utilizando _keos-installer_*.
